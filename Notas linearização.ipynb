{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Macroeconomia de Curto Prazo  \n",
    "FGV EPGE - 2023\n",
    "Ana Paula Nothen Ruhe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RBC Model - Log-linear case"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model conditions:\n",
    "(i) Euler Equation: $c_t^{-\\sigma} = \\beta E\\left[ c_{t+1}^{-\\sigma} \\alpha A_{t+1}k_t^{\\alpha-1}\\right]$  \n",
    "(ii) Feasibility: $c_t + k_t = A_tk_{t-1}^\\alpha$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steady state:\n",
    "* $A^* = 1$;\n",
    "* Euler Equation:  \n",
    "  \n",
    "$(c^*)^{-\\sigma} = \\beta (c^*)^{-\\sigma} \\alpha (k^*)^{\\alpha-1}$  \n",
    "$1 = \\beta\\alpha (k^*)^{\\alpha-1}$  \n",
    "$k^* = \\left(\\frac{1}{\\beta\\alpha}\\right)^\\frac{1}{\\alpha-1}$\n",
    "  \n",
    "* Feasibility:  \n",
    "  \n",
    "$c^* = (k^*)^\\alpha - k^*$  \n",
    "$c^* = k^* \\times \\left( \\frac{1-\\alpha\\beta}{\\alpha\\beta}\\right)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log-linearization:\n",
    "##### The process:\n",
    "* Let $y_t = f(x_t) = ln(x_t)$. By the Taylor rule, we have:  \n",
    "$y_t = y^* + f'(x^*)(x_t-x^*)$  \n",
    "  \n",
    "* With multiple arguments: $y_t = f(x_t,z_t)$:  \n",
    "$y_t = y^* + f_x(x^*,z^*)(x_t-x^*) + f_z(z^*,z^*)(z_t-z^*)$  \n",
    "\n",
    "* We need to apply the log-linearization process to both sides of the conditions.\n",
    "\n",
    "\n",
    "##### Euler Equation:\n",
    "* **LHS:**  $ y_t = ln(c_t^{-\\sigma})$   \n",
    "  - $y^* = -\\sigma ln(c^*)$ \n",
    "\n",
    "$y_t = y^* + \\frac{1}{(c^*)^{-\\sigma}}\\times(-\\sigma {c^*}^{-\\sigma-1} )\\times(c_t - c^*)$  \n",
    "$y_t = y^* - \\sigma \\frac{(c_t - c^*)}{c^*}$  \n",
    "  \n",
    "  .\n",
    "\n",
    " * **RHS:** $w_t = ln(\\beta c_{t+1}^{-\\sigma} \\alpha A_{t+1}k_t^{\\alpha-1})$  \n",
    "   - $w^* = \\ln(\\beta\\alpha) -\\sigma ln(c^*) + (\\alpha-1)ln(k^*)$ \n",
    "   - Given the value of $k^*$, we have $ln(k^*) = \\frac{-1}{\\alpha -1}ln(\\beta\\alpha)$. \n",
    "   - Hence: $w^* = -\\sigma ln(c^*)$ \n",
    "\n",
    " .\n",
    "\n",
    " $w_t = w^* + \\frac{1}{\\beta \\alpha (c^*)^{-\\sigma}(k^*)^{\\alpha-1}} \\times -\\sigma \\beta \\alpha(c^*)^{-\\sigma-1}(k^*)^{\\alpha-1}(c_{t+1} - c^*) + \\frac{1}{\\beta \\alpha (c^*)^{-\\sigma}(k^*)^{\\alpha-1}} \\times 1 \\times (A_{t+1} - A^*) + \\frac{1}{\\beta \\alpha (c^*)^{-\\sigma}(k^*)^{\\alpha-1}} \\times (\\alpha -1) \\beta \\alpha (c^*)^{-\\sigma}(k^*)^{\\alpha-2}\\times(k_t - k^*)$  \n",
    "\n",
    "\n",
    "$w_t = w^* + {-\\sigma}\\frac{(c_{t+1} - c^*)}{c^*} + \\frac{(A_{t+1} - A^*)}{A^*} + (\\alpha -1) \\frac{(k_t - k^*)}{k^*}$\n",
    "  \n",
    "  .\n",
    "\n",
    "* Notation: $\\tilde{x_t} = \\frac{x_t - x^*}{x^*}$.\n",
    "  \n",
    "* (LHS) = (RHS)  \n",
    "\n",
    "$-\\sigma ln(c^*) - \\sigma \\tilde{c_t} = -\\sigma ln(c^*) + {-\\sigma}\\tilde{c}_{t+1} + \\tilde{A}_{t+1} + (\\alpha -1) \\tilde{k}_t$\n",
    "\n",
    "$\\tilde{c_t} = \\tilde{c}_{t+1} - \\frac{1}{\\sigma}  (\\tilde{A}_{t+1} + (\\alpha -1) \\tilde{k}_t)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo em Nível x Linearizado\n",
    "* No Dynare, as IRFs do modelo em nível plotam o valor da variável.  \n",
    "* No modelo linearizado, as IRFs do modelo plotam os desvios do SS em termos percentuais.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metropolis Hastings Algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm\n",
    "\n",
    "#### MCMC: Monte Carlo + Markov Chain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Bayes**: $P(\\theta | y)P(y) = P(\\theta) P(y| \\theta)$.\n",
    "  - $\\theta$ is a vector of parameters.    \n",
    "  - $y$: is a matrix of data.  \n",
    "  - $P(\\theta)$: is the prior distribution $\\rightarrow$ É uma distribuição \"calibrada\", isto é, imposta externamente como chute inicial.\n",
    "  - $P(y| \\theta)$: is the likelihood function.  \n",
    "  - $P(y)$: is the *marginal* likelihood function.  \n",
    "  - Our goal is to estimate the *posterior*, $P(\\theta | y)$.  \n",
    "   \n",
    "* Misto de calibração e estimação tradicional.  \n",
    "  - A dificuldade está no fato de que não observamos a marginal likelihood function.\n",
    "  - $\\theta^0$: chute inicial.\n",
    "  - MCMC: construir uma amostra de valores $\\{\\theta^0, \\theta^1, \\theta^2, ... , \\theta^n \\}$.\n",
    "  - Histograma converge para uma posterior $P(\\theta|y)$. \n",
    "  - Novo $\\theta^{n+1}$ pode ser aceito ou rejeitado $(\\theta^{n+1} = \\theta^n)$: processo iterativo.  \n",
    "$min {\\left\\{\\frac{P(\\theta^{n+1})P(y|\\theta^{n+1})}{P(\\theta^{\\text{rejeita}})P(y|\\theta^{\\text{rejeita}})}, 1\\right\\}} \\geq \\mu \\sim U[0,1]$\n",
    "* $\\{\\theta^i\\} \\rightarrow P(\\theta|y)$.  \n",
    "  - J-scale (input).\n",
    "  - Acceptance rate ($25\\% | 35\\%$)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.1 (tags/v3.11.1:a7a450f, Dec  6 2022, 19:58:39) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
